{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spuco.utils import set_seed\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48004/48004 [00:07<00:00, 6278.40it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 5586.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "import torchvision.transforms as T\n",
    "\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "difficulty = SpuriousFeatureDifficulty.MAGNITUDE_LARGE\n",
    "\n",
    "trainset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    spurious_correlation_strength=0.99,\n",
    "    classes=classes,\n",
    "    split=\"train\",\n",
    "    label_noise=0\n",
    ")\n",
    "trainset.initialize()\n",
    "\n",
    "testset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"test\"\n",
    ")\n",
    "testset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11996/11996 [00:02<00:00, 4708.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "\n",
    "valset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"val\"\n",
    ")\n",
    "valset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDoKYs0TyyRJKjSR43oGBK56ZHbNW7PSJtWlYPI8OnQqZLuWEM0pXsiBRkFucsOQAcckEURere3bXLJb2odVjhs4WDCJQWIyf4nJdiT05wM4yfEVF+z538j8thlsng/rUnvpFJXbs9W+yWpcs7dr2+W1jZEby3mZ5DtREXGWY9hkqPXn0BIhq/JYXWl+HWW5lEN7qk582Moob7NGx2xgdcEEFic/wCsYcZGKFKrTUEl16izDCU8LGnT+21eXbXZfJbjreWe01CC9tbiWGaIMvyN8rqw5VlPBGQD6ggc9c2bnW9euZ0Ees3SebIkawRxRBWZmCgFhGXAJIyQeASeKqUqsVZWUkMpDKR1BHIIohWnGyvoThczxFDlgptQT2T6X1t2LGtGSbxLdG4kM7WccdnDMyLllCKztkDqXYg9vkHAINVqaqKgIRQu5ixwMZJOSfqSST9adU1Z+0m5GWPxTxeInWfX8uh//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACVUlEQVR4Ae2U34sSURTHnSUR0qYf9OMPmKBMsgcVQQVbE/JFcFF0s6X+gEQQxTcpfPJFn0RZCGFZFBa2bM0g8t0JQplCfPWhbKPApw1/TU4nLgx37h12GdreEpHvOed7Ph7vPSOjk7Z0p/1aO23gH95/6F+d6sMt6ddqB9445QweaNU3bn54LgiSJBGNJ1zUum/MXeeJHjm02Wx6vR7CQqEgJ0EcB918IDabTZfLhTfI2nrnU6VSgVAUxX6/L+dBqP/8q9feQYOv4jOZTLgb16lUClWLxeLLF2fxksqkl6+8rdfroVCIZdlut9vpdPAGpM9feG02m0ELglCtVgkDOenFS2/29ve9Xi/4gBiLxb4d+omec+xBdXvbbrfP5/N8Pv/l813CoJgU3HCIiAjuaDR6+PUe0QCh3++HEojRaNQ6YGmDAppOpz0eD5jg7NvtNj0j6jcYDDQIzyiguVwOanCV8Xi837uF+3CNbHiG0Az+1yfpdlerFTiGw2Gj0SCsEDqdzmAwKOfD4fCrpsp6KKBPn3HHTMEwDP7wlMvlTCYjLjfl75CF4vZLpdJyuYQrksu4GAwGvV6P4zi32w352WymSoSSYlKIYaVgmWFDEQ4WFo4C6clk8uP7/UeP12q1Gs/zgUDg59EGKhGfJJQo0yH//onD4YhEIqqnifyK26cRRCa0cWS1WiE5Ho+JEh5qg2az2ROXFOgaoPC8WSwW6JlOp4vFAh+N0BqgiUTCaDRCf6vV+ijcJkB4qAGK2uB5SyaTOILWmm+fRtAZzZPSCDrzT6C/AQpJyD5ogDjtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.ToPILImage()(trainset[10732][0]).resize((28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EnDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x, y, idx\n",
    "\n",
    "train_dataloader = DataLoader(EnDataset(trainset), batch_size=128, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(EnDataset(valset), batch_size=128, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(EnDataset(testset), batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =10\n",
    "early_stopping = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the robust_model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spuco.models import model_factory\n",
    "from torch.optim import Adam\n",
    "model = model_factory(\"lenet\", trainset[0][0].shape, trainset.num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 376/376 [00:05<00:00, 66.50it/s, loss=0.0106, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.1999833277759253\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 376/376 [00:05<00:00, 70.52it/s, loss=0.0141, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.1999833277759253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 376/376 [00:05<00:00, 73.31it/s, loss=0.00226, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.1999833277759253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 376/376 [00:05<00:00, 67.89it/s, loss=0.00327, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.20073357785928642\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 376/376 [00:05<00:00, 69.46it/s, loss=0.0125, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.2543347782594198\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 376/376 [00:05<00:00, 65.25it/s, loss=0.0611, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3693731243747916\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 376/376 [00:05<00:00, 70.51it/s, loss=6.82e-5, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.4502334111370457\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 376/376 [00:05<00:00, 70.18it/s, loss=0.157, acc=1]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5485161720573525\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 376/376 [00:05<00:00, 69.83it/s, loss=0.000345, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5260920306768923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 376/376 [00:05<00:00, 72.96it/s, loss=0.000904, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6242914304768256\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2484383/2809828233.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"output/first_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 10, best validation accuracy: 0.6242914304768256\n",
      "Test accuracy: 0.637\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the forgetting score during training\n",
    "prev_acc = np.zeros(len(trainset))\n",
    "forgetting = np.zeros(len(trainset))\n",
    "\n",
    "kill_cnt = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_dataloader), dynamic_ncols=True) as t:\n",
    "        t.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x, y, idx in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            acc = (y_hat.argmax(dim=1) == y).float()\n",
    "\n",
    "            # Calculate forgetting\n",
    "            for batch_idx, i in enumerate(idx):\n",
    "                if prev_acc[i] > acc[batch_idx]:\n",
    "                    forgetting[i] = forgetting[i]+1\n",
    "            \n",
    "            prev_acc[idx] = acc.cpu().numpy()\n",
    "            acc = acc.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": acc.item()\n",
    "            })\n",
    "            t.update()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    valid_acc = correct/total\n",
    "    print(f\"Validation accuracy: {valid_acc}\")\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        best_epoch = epoch + 1\n",
    "        kill_cnt = 0\n",
    "\n",
    "        torch.save(model.state_dict(), \"output/first_model.pth\")\n",
    "        print(\"Saving model...\")\n",
    "    else:\n",
    "        kill_cnt += 1\n",
    "        if kill_cnt == early_stopping:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(\"output/first_model.pth\"))\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y, idx in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Best epoch: {epoch + 1}, best validation accuracy: {best_acc}\")\n",
    "print(f\"Test accuracy: {correct/total}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating wrong index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "wrong_idx = []\n",
    "for x, y, idx in train_dataloader:\n",
    "    x, y, idx = x.to(device), y.to(device), idx.to(device)\n",
    "    y_hat = model(x)\n",
    "    wrong_batch_idx = (y_hat.argmax(dim=1) != y).nonzero()\n",
    "    wrong_idx.append(idx[wrong_batch_idx].cpu().numpy())\n",
    "\n",
    "wrong_idx = np.concatenate(wrong_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "164\n",
      "0.29878048780487804\n",
      "3.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(len(np.nonzero(forgetting[wrong_idx])[0]))\n",
    "print(len(wrong_idx))\n",
    "print(forgetting[wrong_idx].mean())\n",
    "print(forgetting[wrong_idx].max())\n",
    "print(forgetting[wrong_idx].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JTT, Choose from 1,2,3,4 to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = 100\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Original JTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, upsample_factor)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forgrtting Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample_fgscore = forgetting[wrong_idx]\n",
    "total_repeats = upsample_factor * len(wrong_idx)\n",
    "initial_repeats = wrong_sample_fgscore / wrong_sample_fgscore.sum() * total_repeats\n",
    "repeats = np.round(initial_repeats).astype(int)\n",
    "\n",
    "while repeats.sum() != total_repeats:\n",
    "    diff = total_repeats - repeats.sum()\n",
    "    if diff > 0:\n",
    "        repeats[np.argmax(initial_repeats - repeats)] += 1\n",
    "    else:\n",
    "        repeats[np.argmax(repeats - initial_repeats)] -= 1\n",
    "\n",
    "repeats = repeats.squeeze()\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, repeats)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modified Forgetting Scores (Replace 0 forgetting score as max + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample_fgscore = forgetting[wrong_idx]\n",
    "max_fgscore = wrong_sample_fgscore.max()\n",
    "wrong_sample_fgscore[wrong_sample_fgscore == 0] = max_fgscore\n",
    "\n",
    "total_repeats = upsample_factor * len(wrong_idx)\n",
    "initial_repeats = wrong_sample_fgscore / wrong_sample_fgscore.sum() * total_repeats\n",
    "repeats = np.round(initial_repeats).astype(int)\n",
    "\n",
    "while repeats.sum() != total_repeats:\n",
    "    diff = total_repeats - repeats.sum()\n",
    "    if diff > 0:\n",
    "        repeats[np.argmax(initial_repeats - repeats)] += 1\n",
    "    else:\n",
    "        repeats[np.argmax(repeats - initial_repeats)] -= 1\n",
    "\n",
    "repeats = repeats.squeeze()\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, repeats)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modified Forgetting Scores (Add 1 to all forgetting scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample_fgscore = forgetting[wrong_idx]+1\n",
    "total_repeats = upsample_factor * len(wrong_idx)\n",
    "initial_repeats = wrong_sample_fgscore / wrong_sample_fgscore.sum() * total_repeats\n",
    "repeats = np.round(initial_repeats).astype(int)\n",
    "\n",
    "while repeats.sum() != total_repeats:\n",
    "    diff = total_repeats - repeats.sum()\n",
    "    if diff > 0:\n",
    "        repeats[np.argmax(initial_repeats - repeats)] += 1\n",
    "    else:\n",
    "        repeats[np.argmax(repeats - initial_repeats)] -= 1\n",
    "\n",
    "repeats = repeats.squeeze()\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, repeats)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 74  74  74  74  74 147  74  74 147 294  74  74 147 147  74  74 147  74\n",
      "  74  74  74 147  74  74  74  74  74  74 147  74  74  74 147  74  74  74\n",
      "  74  74  74  74  74  74  74  74  74  74 147  74  74  74  74 147  74  74\n",
      "  74  74  74  74  74  74  74  74 147 147 220  74  74  74 147  74  74  74\n",
      "  74  74  74 147  74 147 147  73 147  73 220  73  73  73 147 147 147 147\n",
      "  73 147  73  73  73  73  73 220  73  73 147  73  73  73  73 147  73  73\n",
      "  73  73  73 147  73  73  73 147  73 147 147  73  73  73 220  73 147  73\n",
      " 147  73  73  73 147 147 147  73 147  73  73  73  73  73  73  73  73 147\n",
      " 220 220  73  73  73  73  73  73  73  73  73  73  73  73  73  73  73 147\n",
      " 294  73  73  73 147  73  73  73  73  73  73  73  73 147 147 220 147  73\n",
      "  73  73 147  73  73  73  73  73  73 220 220  73  73 147 147  73  73  73\n",
      " 147  73  73  73  73  73  73 147  73 147 147  73  73  73  73 220  73 147\n",
      "  73  73 220  73  73 147  73  73  73 220  73 147 147 147 147  73 220  73\n",
      "  73  73  73  73  73  73  73  73  73 147  73 220  73  73  73  73 147  73\n",
      "  73  73 147  73  73 220 147 147 147  73]\n"
     ]
    }
   ],
   "source": [
    "print(repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "early_stopping = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(EnDataset(upsample_trainset), batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_model = model_factory(\"lenet\", trainset[0][0].shape, trainset.num_classes).to(device)\n",
    "optimizer = Adam(robust_model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 504/504 [00:03<00:00, 149.72it/s, loss=0.187, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3911303767922641\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 504/504 [00:03<00:00, 158.62it/s, loss=0.178, acc=0.95]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5067522507502501\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 504/504 [00:03<00:00, 151.10it/s, loss=0.168, acc=0.95]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5294264754918306\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 504/504 [00:03<00:00, 144.81it/s, loss=0.00503, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5419306435478493\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 504/504 [00:03<00:00, 148.28it/s, loss=0.00855, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5731077025675225\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 504/504 [00:03<00:00, 141.98it/s, loss=0.00519, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5798599533177726\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 504/504 [00:03<00:00, 157.68it/s, loss=0.00185, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6465488496165388\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 504/504 [00:03<00:00, 160.02it/s, loss=0.0108, acc=1]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.653134378126042\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 504/504 [00:03<00:00, 160.94it/s, loss=0.00168, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6941480493497832\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 504/504 [00:03<00:00, 152.78it/s, loss=0.000113, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6202900966988997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 504/504 [00:03<00:00, 155.93it/s, loss=0.000121, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6407135711903968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 504/504 [00:03<00:00, 152.51it/s, loss=0.000852, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.660053351117039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 504/504 [00:03<00:00, 156.04it/s, loss=0.000654, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6707235745248417\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2484383/3517860651.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  robust_model.load_state_dict(torch.load(\"output/robust_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 13, best validation accuracy: 0.6941480493497832\n",
      "Test accuracy: 0.7029\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "kill_cnt = 0\n",
    "best_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "    robust_model.train()\n",
    "    with tqdm(total=len(train_dataloader), dynamic_ncols=True) as t:\n",
    "        t.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x, y, idx in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = robust_model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            acc = (y_hat.argmax(dim=1) == y).float()\n",
    "            acc = acc.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": acc.item()\n",
    "            })\n",
    "            t.update()\n",
    "\n",
    "    robust_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = robust_model(x)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    valid_acc = correct/total\n",
    "    print(f\"Validation accuracy: {valid_acc}\")\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        best_epoch = epoch + 1\n",
    "        kill_cnt = 0\n",
    "\n",
    "        torch.save(robust_model.state_dict(), \"output/robust_model.pth\")\n",
    "        print(\"Saving model...\")\n",
    "    else:\n",
    "        kill_cnt += 1\n",
    "        if kill_cnt == early_stopping:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "robust_model.eval()\n",
    "robust_model.load_state_dict(torch.load(\"output/robust_model.pth\"))\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y, idx in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = robust_model(x)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Best epoch: {epoch + 1}, best validation accuracy: {best_acc}\")\n",
    "print(f\"Test accuracy: {correct/total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "260D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
