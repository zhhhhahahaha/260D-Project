{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spuco.utils import set_seed\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48004/48004 [00:10<00:00, 4697.34it/s]\n",
      "100%|██████████| 10000/10000 [00:02<00:00, 4196.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "import torchvision.transforms as T\n",
    "\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "difficulty = SpuriousFeatureDifficulty.MAGNITUDE_LARGE\n",
    "\n",
    "trainset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    spurious_correlation_strength=0.95,\n",
    "    classes=classes,\n",
    "    split=\"train\",\n",
    "    label_noise=0\n",
    ")\n",
    "trainset.initialize()\n",
    "\n",
    "testset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"test\"\n",
    ")\n",
    "testset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 7362/11996 [00:01<00:00, 4715.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11996/11996 [00:02<00:00, 4752.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "\n",
    "valset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"val\"\n",
    ")\n",
    "valset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDoKYs0TyyRJKjSR43oGBK56ZHbNW7PSJtWlYPI8OnQqZLuWEM0pXsiBRkFucsOQAcckEURere3bXLJb2odVjhs4WDCJQWIyf4nJdiT05wM4yfEVF+z538j8thlsng/rUnvpFJXbs9W+yWpcs7dr2+W1jZEby3mZ5DtREXGWY9hkqPXn0BIhq/JYXWl+HWW5lEN7qk582Moob7NGx2xgdcEEFic/wCsYcZGKFKrTUEl16izDCU8LGnT+21eXbXZfJbjreWe01CC9tbiWGaIMvyN8rqw5VlPBGQD6ggc9c2bnW9euZ0Ees3SebIkawRxRBWZmCgFhGXAJIyQeASeKqUqsVZWUkMpDKR1BHIIohWnGyvoThczxFDlgptQT2T6X1t2LGtGSbxLdG4kM7WccdnDMyLllCKztkDqXYg9vkHAINVqaqKgIRQu5ixwMZJOSfqSST9adU1Z+0m5GWPxTxeInWfX8uh//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACVUlEQVR4Ae2U34sSURTHnSUR0qYf9OMPmKBMsgcVQQVbE/JFcFF0s6X+gEQQxTcpfPJFn0RZCGFZFBa2bM0g8t0JQplCfPWhbKPApw1/TU4nLgx37h12GdreEpHvOed7Ph7vPSOjk7Z0p/1aO23gH95/6F+d6sMt6ddqB9445QweaNU3bn54LgiSJBGNJ1zUum/MXeeJHjm02Wx6vR7CQqEgJ0EcB918IDabTZfLhTfI2nrnU6VSgVAUxX6/L+dBqP/8q9feQYOv4jOZTLgb16lUClWLxeLLF2fxksqkl6+8rdfroVCIZdlut9vpdPAGpM9feG02m0ELglCtVgkDOenFS2/29ve9Xi/4gBiLxb4d+omec+xBdXvbbrfP5/N8Pv/l813CoJgU3HCIiAjuaDR6+PUe0QCh3++HEojRaNQ6YGmDAppOpz0eD5jg7NvtNj0j6jcYDDQIzyiguVwOanCV8Xi837uF+3CNbHiG0Az+1yfpdlerFTiGw2Gj0SCsEDqdzmAwKOfD4fCrpsp6KKBPn3HHTMEwDP7wlMvlTCYjLjfl75CF4vZLpdJyuYQrksu4GAwGvV6P4zi32w352WymSoSSYlKIYaVgmWFDEQ4WFo4C6clk8uP7/UeP12q1Gs/zgUDg59EGKhGfJJQo0yH//onD4YhEIqqnifyK26cRRCa0cWS1WiE5Ho+JEh5qg2az2ROXFOgaoPC8WSwW6JlOp4vFAh+N0BqgiUTCaDRCf6vV+ijcJkB4qAGK2uB5SyaTOILWmm+fRtAZzZPSCDrzT6C/AQpJyD5ogDjtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.ToPILImage()(trainset[10732][0]).resize((28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EnDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x, y, idx\n",
    "\n",
    "train_dataloader = DataLoader(EnDataset(trainset), batch_size=128, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(EnDataset(valset), batch_size=128, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(EnDataset(testset), batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the robust_model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spuco.models import model_factory\n",
    "from torch.optim import Adam\n",
    "model = model_factory(\"lenet\", trainset[0][0].shape, trainset.num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10: 100%|██████████| 376/376 [00:07<00:00, 50.02it/s, loss=0.0412, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.2031510503501167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 376/376 [00:07<00:00, 53.47it/s, loss=0.00854, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3995498499499833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 376/376 [00:06<00:00, 54.94it/s, loss=0.00629, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.612704234744915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 376/376 [00:07<00:00, 53.03it/s, loss=0.000753, acc=1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7528342780926975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 376/376 [00:07<00:00, 53.03it/s, loss=0.0778, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7930976992330777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 376/376 [00:07<00:00, 53.44it/s, loss=0.000116, acc=1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.832110703567856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 376/376 [00:07<00:00, 52.98it/s, loss=2.4e-5, acc=1]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8658719573191064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 376/376 [00:06<00:00, 54.45it/s, loss=0.00101, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8738746248749584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 376/376 [00:06<00:00, 54.73it/s, loss=0.000555, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8537012337445815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 376/376 [00:06<00:00, 54.56it/s, loss=6.11e-5, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9023007669223074\n",
      "Test accuracy: 0.9099\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the forgetting score during training\n",
    "prev_acc = np.zeros(len(trainset))\n",
    "forgetting = np.zeros(len(trainset))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_dataloader), dynamic_ncols=True) as t:\n",
    "        t.set_description(f\"Epoch {epoch}/{epochs}\")\n",
    "        for x, y, idx in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            acc = (y_hat.argmax(dim=1) == y).float()\n",
    "\n",
    "            # Calculate forgetting\n",
    "            for batch_idx, i in enumerate(idx):\n",
    "                if prev_acc[i] > acc[batch_idx]:\n",
    "                    forgetting[i] = forgetting[i]+1\n",
    "            \n",
    "            prev_acc[idx] = acc.cpu().numpy()\n",
    "            acc = acc.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": acc.item()\n",
    "            })\n",
    "            t.update()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(f\"Validation accuracy: {correct/total}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y, idx in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Test accuracy: {correct/total}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating wrong index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "wrong_idx = []\n",
    "for x, y, idx in train_dataloader:\n",
    "    x, y, idx = x.to(device), y.to(device), idx.to(device)\n",
    "    y_hat = model(x)\n",
    "    wrong_batch_idx = (y_hat.argmax(dim=1) != y).nonzero()\n",
    "    wrong_idx.append(idx[wrong_batch_idx].cpu().numpy())\n",
    "\n",
    "wrong_idx = np.concatenate(wrong_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "189\n",
      "0.7037037037037037\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(len(np.nonzero(forgetting[wrong_idx])[0]))\n",
    "print(len(wrong_idx))\n",
    "print(forgetting[wrong_idx].mean())\n",
    "print(forgetting[wrong_idx].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JTT, Choose from 1,2 to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Original JTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 47953, 47953, 47953])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, upsample_factor)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Based on Forgrtting Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample_fgscore = forgetting[wrong_idx]\n",
    "total_repeats = upsample_factor * len(wrong_idx)\n",
    "initial_repeats = wrong_sample_fgscore / wrong_sample_fgscore.sum() * total_repeats\n",
    "repeats = np.round(initial_repeats).astype(int)\n",
    "\n",
    "while repeats.sum() != total_repeats:\n",
    "    diff = total_repeats - repeats.sum()\n",
    "    if diff > 0:\n",
    "        repeats[np.argmax(initial_repeats - repeats)] += 1\n",
    "    else:\n",
    "        repeats[np.argmax(repeats - initial_repeats)] -= 1\n",
    "\n",
    "repeats = repeats.squeeze()\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, repeats)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(EnDataset(upsample_trainset), batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_model = model_factory(\"lenet\", trainset[0][0].shape, trainset.num_classes).to(device)\n",
    "optimizer = Adam(robust_model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10: 100%|██████████| 390/390 [00:04<00:00, 90.56it/s, loss=0.132, acc=0.971]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.22074024674891632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 390/390 [00:04<00:00, 90.31it/s, loss=0.0911, acc=0.971] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.41980660220073357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 390/390 [00:04<00:00, 90.46it/s, loss=0.0669, acc=0.98]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5894464821607203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 390/390 [00:04<00:00, 90.51it/s, loss=0.0762, acc=0.961] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7139879959986662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 390/390 [00:04<00:00, 91.89it/s, loss=0.149, acc=0.971]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7206568856285428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 390/390 [00:04<00:00, 87.97it/s, loss=0.0185, acc=0.99]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8055185061687229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 390/390 [00:04<00:00, 90.33it/s, loss=0.0675, acc=0.98]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.823191063687896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 390/390 [00:04<00:00, 90.51it/s, loss=0.0178, acc=1]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8617872624208069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 390/390 [00:04<00:00, 91.90it/s, loss=0.0183, acc=0.99]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8562020673557853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 390/390 [00:04<00:00, 91.91it/s, loss=0.0636, acc=0.98]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8746248749583194\n",
      "Test accuracy: 0.8844\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    robust_model.train()\n",
    "    with tqdm(total=len(train_dataloader), dynamic_ncols=True) as t:\n",
    "        t.set_description(f\"Epoch {epoch}/{epochs}\")\n",
    "        for x, y, idx in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = robust_model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            acc = (y_hat.argmax(dim=1) == y).float()\n",
    "            acc = acc.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": acc.item()\n",
    "            })\n",
    "            t.update()\n",
    "\n",
    "    robust_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = robust_model(x)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(f\"Validation accuracy: {correct/total}\")\n",
    "\n",
    "robust_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y, idx in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = robust_model(x)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Test accuracy: {correct/total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "260D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
