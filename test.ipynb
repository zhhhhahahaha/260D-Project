{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhzhang/miniconda3/envs/260D/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from spuco.utils import set_seed\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48004/48004 [00:07<00:00, 6087.04it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6523.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "import torchvision.transforms as T\n",
    "\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "difficulty = SpuriousFeatureDifficulty.MAGNITUDE_LARGE\n",
    "\n",
    "trainset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    spurious_correlation_strength=0.99,\n",
    "    classes=classes,\n",
    "    split=\"train\",\n",
    "    label_noise=0\n",
    ")\n",
    "trainset.initialize()\n",
    "\n",
    "testset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"test\"\n",
    ")\n",
    "testset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11996 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11996/11996 [00:01<00:00, 6247.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "\n",
    "valset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"val\"\n",
    ")\n",
    "valset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDoKYs0TyyRJKjSR43oGBK56ZHbNW7PSJtWlYPI8OnQqZLuWEM0pXsiBRkFucsOQAcckEURere3bXLJb2odVjhs4WDCJQWIyf4nJdiT05wM4yfEVF+z538j8thlsng/rUnvpFJXbs9W+yWpcs7dr2+W1jZEby3mZ5DtREXGWY9hkqPXn0BIhq/JYXWl+HWW5lEN7qk582Moob7NGx2xgdcEEFic/wCsYcZGKFKrTUEl16izDCU8LGnT+21eXbXZfJbjreWe01CC9tbiWGaIMvyN8rqw5VlPBGQD6ggc9c2bnW9euZ0Ees3SebIkawRxRBWZmCgFhGXAJIyQeASeKqUqsVZWUkMpDKR1BHIIohWnGyvoThczxFDlgptQT2T6X1t2LGtGSbxLdG4kM7WccdnDMyLllCKztkDqXYg9vkHAINVqaqKgIRQu5ixwMZJOSfqSST9adU1Z+0m5GWPxTxeInWfX8uh//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACVUlEQVR4Ae2U34sSURTHnSUR0qYf9OMPmKBMsgcVQQVbE/JFcFF0s6X+gEQQxTcpfPJFn0RZCGFZFBa2bM0g8t0JQplCfPWhbKPApw1/TU4nLgx37h12GdreEpHvOed7Ph7vPSOjk7Z0p/1aO23gH95/6F+d6sMt6ddqB9445QweaNU3bn54LgiSJBGNJ1zUum/MXeeJHjm02Wx6vR7CQqEgJ0EcB918IDabTZfLhTfI2nrnU6VSgVAUxX6/L+dBqP/8q9feQYOv4jOZTLgb16lUClWLxeLLF2fxksqkl6+8rdfroVCIZdlut9vpdPAGpM9feG02m0ELglCtVgkDOenFS2/29ve9Xi/4gBiLxb4d+omec+xBdXvbbrfP5/N8Pv/l813CoJgU3HCIiAjuaDR6+PUe0QCh3++HEojRaNQ6YGmDAppOpz0eD5jg7NvtNj0j6jcYDDQIzyiguVwOanCV8Xi837uF+3CNbHiG0Az+1yfpdlerFTiGw2Gj0SCsEDqdzmAwKOfD4fCrpsp6KKBPn3HHTMEwDP7wlMvlTCYjLjfl75CF4vZLpdJyuYQrksu4GAwGvV6P4zi32w352WymSoSSYlKIYaVgmWFDEQ4WFo4C6clk8uP7/UeP12q1Gs/zgUDg59EGKhGfJJQo0yH//onD4YhEIqqnifyK26cRRCa0cWS1WiE5Ho+JEh5qg2az2ROXFOgaoPC8WSwW6JlOp4vFAh+N0BqgiUTCaDRCf6vV+ijcJkB4qAGK2uB5SyaTOILWmm+fRtAZzZPSCDrzT6C/AQpJyD5ogDjtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.ToPILImage()(trainset[10732][0]).resize((28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EnDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x, y, idx\n",
    "\n",
    "train_dataloader = DataLoader(EnDataset(trainset), batch_size=128, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(EnDataset(valset), batch_size=128, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(EnDataset(testset), batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the robust_model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spuco.models import model_factory\n",
    "from torch.optim import Adam\n",
    "model = model_factory(\"lenet\", trainset[0][0].shape, trainset.num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 376/376 [00:06<00:00, 61.30it/s, loss=0.0106, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.1999833277759253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 376/376 [00:05<00:00, 66.37it/s, loss=0.0141, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.1999833277759253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 376/376 [00:05<00:00, 70.67it/s, loss=0.00226, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.1999833277759253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 376/376 [00:05<00:00, 68.05it/s, loss=0.00327, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.20073357785928642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 376/376 [00:05<00:00, 64.14it/s, loss=0.0125, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.2543347782594198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 376/376 [00:05<00:00, 64.53it/s, loss=0.0611, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3693731243747916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 376/376 [00:05<00:00, 65.61it/s, loss=6.82e-5, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.4502334111370457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 376/376 [00:05<00:00, 64.65it/s, loss=0.157, acc=1]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5485161720573525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 376/376 [00:05<00:00, 67.70it/s, loss=0.000345, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5260920306768923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 376/376 [00:05<00:00, 67.54it/s, loss=0.000904, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6242914304768256\n",
      "Test accuracy: 0.637\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the forgetting score during training\n",
    "prev_acc = np.zeros(len(trainset))\n",
    "forgetting = np.zeros(len(trainset))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_dataloader), dynamic_ncols=True) as t:\n",
    "        t.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x, y, idx in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            acc = (y_hat.argmax(dim=1) == y).float()\n",
    "\n",
    "            # Calculate forgetting\n",
    "            for batch_idx, i in enumerate(idx):\n",
    "                if prev_acc[i] > acc[batch_idx]:\n",
    "                    forgetting[i] = forgetting[i]+1\n",
    "            \n",
    "            prev_acc[idx] = acc.cpu().numpy()\n",
    "            acc = acc.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": acc.item()\n",
    "            })\n",
    "            t.update()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(f\"Validation accuracy: {correct/total}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y, idx in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Test accuracy: {correct/total}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating wrong index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "wrong_idx = []\n",
    "for x, y, idx in train_dataloader:\n",
    "    x, y, idx = x.to(device), y.to(device), idx.to(device)\n",
    "    y_hat = model(x)\n",
    "    wrong_batch_idx = (y_hat.argmax(dim=1) != y).nonzero()\n",
    "    wrong_idx.append(idx[wrong_batch_idx].cpu().numpy())\n",
    "\n",
    "wrong_idx = np.concatenate(wrong_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "164\n",
      "0.29878048780487804\n",
      "3.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(len(np.nonzero(forgetting[wrong_idx])[0]))\n",
    "print(len(wrong_idx))\n",
    "print(forgetting[wrong_idx].mean())\n",
    "print(forgetting[wrong_idx].max())\n",
    "print(forgetting[wrong_idx].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JTT, Choose from 1,2 to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Original JTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, upsample_factor)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Based on Forgrtting Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample_fgscore = forgetting[wrong_idx]+1\n",
    "total_repeats = upsample_factor * len(wrong_idx)\n",
    "initial_repeats = wrong_sample_fgscore / wrong_sample_fgscore.sum() * total_repeats\n",
    "repeats = np.round(initial_repeats).astype(int)\n",
    "\n",
    "while repeats.sum() != total_repeats:\n",
    "    diff = total_repeats - repeats.sum()\n",
    "    if diff > 0:\n",
    "        repeats[np.argmax(initial_repeats - repeats)] += 1\n",
    "    else:\n",
    "        repeats[np.argmax(repeats - initial_repeats)] -= 1\n",
    "\n",
    "repeats = repeats.squeeze()\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, repeats)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 23 23 23 23 23 23 23 47 23 23 47 23 23 23 23 23 23 23 47 23 23 23 47\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 47 23 23 47 23 23 47 47 23\n",
      " 23 23 23 47 47 23 23 23 47 23 23 23 23 23 23 23 23 23 23 23 23 47 23 47\n",
      " 23 23 23 46 23 23 23 23 70 23 46 23 23 46 46 23 23 23 23 46 23 23 23 23\n",
      " 23 23 23 46 46 23 70 23 23 23 23 23 93 46 23 23 23 46 23 23 46 23 23 23\n",
      " 23 23 70 23 23 23 23 23 23 46 46 23 23 23 70 23 23 23 46 23 23 23 23 23\n",
      " 46 93 23 46 23 46 23 23 23 23 23 23 23 46 70 23 46 70 23 23]\n"
     ]
    }
   ],
   "source": [
    "print(repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "early_stopping = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(EnDataset(upsample_trainset), batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_model = model_factory(\"lenet\", trainset[0][0].shape, trainset.num_classes).to(device)\n",
    "optimizer = Adam(robust_model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 414/414 [00:02<00:00, 150.27it/s, loss=0.338, acc=0.9]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.24924974991663887\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 414/414 [00:02<00:00, 138.52it/s, loss=0.207, acc=0.95]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.35245081693897967\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 414/414 [00:02<00:00, 158.34it/s, loss=0.0649, acc=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5074191397132377\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 414/414 [00:02<00:00, 145.07it/s, loss=0.0184, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5180893631210404\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 414/414 [00:03<00:00, 133.07it/s, loss=0.0221, acc=0.983] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6545515171723908\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 414/414 [00:02<00:00, 138.80it/s, loss=0.0028, acc=1]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6589696565521841\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 414/414 [00:03<00:00, 128.76it/s, loss=0.00174, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6182894298099366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 414/414 [00:02<00:00, 139.18it/s, loss=0.00172, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6300433477825942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 414/414 [00:02<00:00, 147.21it/s, loss=0.00102, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6703067689229744\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 414/414 [00:02<00:00, 158.89it/s, loss=0.00549, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6495498499499833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 414/414 [00:02<00:00, 159.83it/s, loss=0.00174, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7089029676558853\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 414/414 [00:02<00:00, 139.84it/s, loss=0.000178, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6753084361453818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 414/414 [00:02<00:00, 150.71it/s, loss=0.0414, acc=0.983] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6853117705901968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 414/414 [00:02<00:00, 149.75it/s, loss=7.35e-5, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6628042680893631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 414/414 [00:02<00:00, 139.27it/s, loss=0.00519, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6845615205068356\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_647823/3517860651.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  robust_model.load_state_dict(torch.load(\"output/robust_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 15, best validation accuracy: 0.7089029676558853\n",
      "Test accuracy: 0.708\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "kill_cnt = 0\n",
    "best_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "    robust_model.train()\n",
    "    with tqdm(total=len(train_dataloader), dynamic_ncols=True) as t:\n",
    "        t.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x, y, idx in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = robust_model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            acc = (y_hat.argmax(dim=1) == y).float()\n",
    "            acc = acc.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": acc.item()\n",
    "            })\n",
    "            t.update()\n",
    "\n",
    "    robust_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = robust_model(x)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    valid_acc = correct/total\n",
    "    print(f\"Validation accuracy: {valid_acc}\")\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        best_epoch = epoch + 1\n",
    "        kill_cnt = 0\n",
    "\n",
    "        torch.save(robust_model.state_dict(), \"output/robust_model.pth\")\n",
    "        print(\"Saving model...\")\n",
    "    else:\n",
    "        kill_cnt += 1\n",
    "        if kill_cnt == early_stopping:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "robust_model.eval()\n",
    "robust_model.load_state_dict(torch.load(\"output/robust_model.pth\"))\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y, idx in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = robust_model(x)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Best epoch: {epoch + 1}, best validation accuracy: {best_acc}\")\n",
    "print(f\"Test accuracy: {correct/total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "260D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
