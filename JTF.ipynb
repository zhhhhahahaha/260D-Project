{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhzhang/miniconda3/envs/260D/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from spuco.utils import set_seed\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48004/48004 [00:09<00:00, 5239.18it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 5220.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "import torchvision.transforms as T\n",
    "\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "difficulty = SpuriousFeatureDifficulty.MAGNITUDE_LARGE\n",
    "\n",
    "trainset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    spurious_correlation_strength=0.98,\n",
    "    classes=classes,\n",
    "    split=\"train\",\n",
    "    label_noise=0\n",
    ")\n",
    "trainset.initialize()\n",
    "\n",
    "testset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"test\"\n",
    ")\n",
    "testset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11996/11996 [00:02<00:00, 5502.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "\n",
    "valset = SpuCoMNIST(\n",
    "    root=\"../data/mnist/\",\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"val\"\n",
    ")\n",
    "valset.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1Sq096kUhgjjlubrbuFtAu6QjBxkdFBKkBmIXPGas1hafPNd2Vxa6VcrJpssru8mn2/kmVpJCxxcO+x+pU+X82TwVIwPynKcFSxdRqrzWXSKu389l6slGnDdyPci3uLOa0laISoszxNvGcNjY7Z2nGfTcvrVqsO2tYdL1jS9HjiWB7ezvZ2hG1RGs1wjKoAPOMEHbkLxnG5c7lLOMJTwuKdOlorJ2fS62BlDVba0vY7e21CW6WykmAnjt4Xfz12sfLfaCVQkDJ4BxtJw2Dz62dzcagLmOKS/ukmEUUs9kbSys40wygRMweQL1XaArPglvkUr19Fb4LO6uDw3sKUFe97v+unS4XM2fSjdyI15dzXhR98ZuOHhJXaxjkj2Om7qQGxyQABgC7bRNBaQxO+90RVZvm+YgYz8xJ/Mk+pPWpaK4MRj8RiYqNaXNbva/37gf/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACnElEQVR4AWO88J+B6oCJ6iYCDRzhhrKQFKa2fLw8PDze3t5iYmK9vb0nf/zEqp1YQwOVFMvKyg4fttTR0YEYJCEhwcCQh9VQRoJJKkJTo6CgICYmhoODg5GR8fHjx58/f9bU1Hzz5o2Dg8OK6zcwzcXnUnsB/s7OzuPHw3l5eSE6b9++7e7uzsbGdv36dREwwDQRKIIvSQUGBqakpMBNvHv3rqur68YHD7EahCyIz9DQ0FCI0gcPHqxevRpo4uZHj4EiGhoayEZgsvF5PzU1NS0tbdeuXXfu3Nn18tWmCKh2cXFxTIOQRfAZuu3pMwaGhoB6ZPUgtqWlJboQKh+focgq5+XncXNzA2P/////gYG6QKljx44dP34cWQ2cTSBJWXFzaWtr19XVeXl5AfUwMTH9+/cPyHj+/Lm9vf3aO3fhBiEzcEaUKRtrmrnZzZs3gc4BpkegKWvXrgWmUKBLgYCZmTkoKMiMnQ3ZLAQbmPgxETAl+vn5/QGD2tpaa2troBohIaELFy5ABCFkeHg4Ozs7pnYsJrKysra3t0O0bdmyRUBAAKhNVFT09OnTf//+/f79e2NjI9DVEAU7duxwcnIyBAO46eiGAv3V0dEB1PDx48esrCxBQUGgUlNT0xMnTgAFb9y44ejoCBTh4+Pz8PBYvHgxUBnE9Pv37+M0NDMzE6jo06dPERERQP96enquWrXqy5cvQEFgdMnKysJ1QhiRkZFA3wCBqqoqXAo99sVfPAf69OfPn0BHAdOQiooKJPgbGhqAYXL2z19EbOBmoRv6/8J5XV1QMoSAbdu2HTp0aMOGDcCceub3H5gwARrdUGAxHBAQYGRk9OrVq3nz5r1///7Uz18EzMCQRjcUQwE5AjgTPzmGwfSMcEMBDmBtHE/fHroAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.ToPILImage()(trainset[5][0]).resize((28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EnDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x, y, idx\n",
    "\n",
    "train_dataloader = DataLoader(EnDataset(trainset), batch_size=128, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(EnDataset(valset), batch_size=128, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(EnDataset(testset), batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =10\n",
    "early_stopping = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the robust_model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spuco.models import model_factory\n",
    "from torch.optim import Adam\n",
    "model = model_factory(\"lenet\", trainset[0][0].shape, trainset.num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 376/376 [00:12<00:00, 29.49it/s, loss=0.0247, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.1999833277759253\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 376/376 [00:04<00:00, 78.04it/s, loss=0.0131, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.20231743914638212\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 376/376 [00:05<00:00, 68.30it/s, loss=0.00424, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.2557519173057686\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 376/376 [00:04<00:00, 79.47it/s, loss=0.00107, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3849616538846282\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 376/376 [00:04<00:00, 77.20it/s, loss=0.209, acc=0.75]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.4968322774258086\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 376/376 [00:04<00:00, 77.06it/s, loss=0.00419, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6142880960320106\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 376/376 [00:05<00:00, 73.23it/s, loss=2.1e-5, acc=1]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6702234078026008\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 376/376 [00:06<00:00, 61.40it/s, loss=0.00244, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7580860286762254\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 376/376 [00:04<00:00, 76.90it/s, loss=0.00022, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7359119706568856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 376/376 [00:05<00:00, 74.84it/s, loss=0.000162, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7871790596865622\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_429534/2809828233.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"output/first_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 10, best validation accuracy: 0.7871790596865622\n",
      "Test accuracy: 0.7979\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the forgetting score during training\n",
    "prev_acc = np.zeros(len(trainset))\n",
    "forgetting = np.zeros(len(trainset))\n",
    "\n",
    "kill_cnt = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_dataloader), dynamic_ncols=True) as t:\n",
    "        t.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x, y, idx in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            acc = (y_hat.argmax(dim=1) == y).float()\n",
    "\n",
    "            # Calculate forgetting\n",
    "            for batch_idx, i in enumerate(idx):\n",
    "                if prev_acc[i] > acc[batch_idx]:\n",
    "                    forgetting[i] = forgetting[i]+1\n",
    "            \n",
    "            prev_acc[idx] = acc.cpu().numpy()\n",
    "            acc = acc.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": acc.item()\n",
    "            })\n",
    "            t.update()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    valid_acc = correct/total\n",
    "    print(f\"Validation accuracy: {valid_acc}\")\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        best_epoch = epoch + 1\n",
    "        kill_cnt = 0\n",
    "\n",
    "        torch.save(model.state_dict(), \"output/first_model.pth\")\n",
    "        print(\"Saving model...\")\n",
    "    else:\n",
    "        kill_cnt += 1\n",
    "        if kill_cnt == early_stopping:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(\"output/first_model.pth\"))\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y, idx in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Best epoch: {epoch + 1}, best validation accuracy: {best_acc}\")\n",
    "print(f\"Test accuracy: {correct/total}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spuco.evaluate import Evaluator\n",
    "\n",
    "# evaluator = Evaluator(\n",
    "#     testset=testset,\n",
    "#     group_partition=testset.group_partition,\n",
    "#     group_weights=trainset.group_weights,\n",
    "#     batch_size=64,\n",
    "#     model=model,\n",
    "#     device=device,\n",
    "#     verbose=True\n",
    "# )\n",
    "# evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating wrong index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "wrong_idx = []\n",
    "for x, y, idx in train_dataloader:\n",
    "    x, y, idx = x.to(device), y.to(device), idx.to(device)\n",
    "    y_hat = model(x)\n",
    "    wrong_batch_idx = (y_hat.argmax(dim=1) != y).nonzero()\n",
    "    wrong_idx.append(idx[wrong_batch_idx].cpu().numpy())\n",
    "\n",
    "wrong_idx = np.concatenate(wrong_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "262\n",
      "0.36259541984732824\n",
      "3.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(len(np.nonzero(forgetting[wrong_idx])[0]))\n",
    "print(len(wrong_idx))\n",
    "print(forgetting[wrong_idx].mean())\n",
    "print(forgetting[wrong_idx].max())\n",
    "print(forgetting[wrong_idx].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JTT, Choose from 1,2,3,4 to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_factor = 100\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Original JTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, upsample_factor)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forgrtting Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample_fgscore = forgetting[wrong_idx]\n",
    "total_repeats = upsample_factor * len(wrong_idx)\n",
    "initial_repeats = wrong_sample_fgscore / wrong_sample_fgscore.sum() * total_repeats\n",
    "repeats = np.round(initial_repeats).astype(int)\n",
    "\n",
    "while repeats.sum() != total_repeats:\n",
    "    diff = total_repeats - repeats.sum()\n",
    "    if diff > 0:\n",
    "        repeats[np.argmax(initial_repeats - repeats)] += 1\n",
    "    else:\n",
    "        repeats[np.argmax(repeats - initial_repeats)] -= 1\n",
    "\n",
    "repeats = repeats.squeeze()\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, repeats)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modified Forgetting Scores (Replace 0 forgetting score as max + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample_fgscore = forgetting[wrong_idx]\n",
    "max_fgscore = wrong_sample_fgscore.max()\n",
    "wrong_sample_fgscore[wrong_sample_fgscore == 0] = max_fgscore+1\n",
    "\n",
    "total_repeats = upsample_factor * len(wrong_idx)\n",
    "initial_repeats = wrong_sample_fgscore / wrong_sample_fgscore.sum() * total_repeats\n",
    "repeats = np.round(initial_repeats).astype(int)\n",
    "\n",
    "while repeats.sum() != total_repeats:\n",
    "    diff = total_repeats - repeats.sum()\n",
    "    if diff > 0:\n",
    "        repeats[np.argmax(initial_repeats - repeats)] += 1\n",
    "    else:\n",
    "        repeats[np.argmax(repeats - initial_repeats)] -= 1\n",
    "\n",
    "repeats = repeats.squeeze()\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, repeats)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modified Forgetting Scores (Add 1 to all forgetting scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample_fgscore = forgetting[wrong_idx]+1\n",
    "total_repeats = upsample_factor * len(wrong_idx)\n",
    "initial_repeats = wrong_sample_fgscore / wrong_sample_fgscore.sum() * total_repeats\n",
    "repeats = np.round(initial_repeats).astype(int)\n",
    "\n",
    "while repeats.sum() != total_repeats:\n",
    "    diff = total_repeats - repeats.sum()\n",
    "    if diff > 0:\n",
    "        repeats[np.argmax(initial_repeats - repeats)] += 1\n",
    "    else:\n",
    "        repeats[np.argmax(repeats - initial_repeats)] -= 1\n",
    "\n",
    "repeats = repeats.squeeze()\n",
    "upsample_idx = np.concat((np.arange(len(trainset)), np.repeat(wrong_idx, repeats)))\n",
    "upsample_trainset = Subset(trainset, upsample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125  31  31  31  31 125  31 125 125 125 125 125  31  31  31 125  31  31\n",
      "  31  31 125 125  31 125 125  31 125 125 125 125 125 125  63  63 125 125\n",
      " 125  31 125  31 125 125 125 125  31 125 125 125  31 125 125 125 125  31\n",
      "  31 125 125 125 125 125 125  63 125 125 125 125 125 125 125 125 125  31\n",
      " 125 125 125 125 125  31 125 125 125 125 125 125 125  31  31 125  31 125\n",
      " 125 125  31 125  31 125  31 125 125 125 125 125 125 125  31  31  31 125\n",
      " 125 125 125  31 125 125 125  31 125  31 125 125  31 125  31 125  62 125\n",
      " 125  31 125 125  31 125  62  62 125 125  31 125 125 125 125 125 125 125\n",
      " 125  31 125 125 125 125  31 125  31  94 125  31 125  31 125 125  62 125\n",
      " 125 125 125  31 125 125 125  31 125 125 125 125  62 125 125 125 125 125\n",
      " 125  31 125 125 125 125  31 125  31 125 125 125 125  31 125 125  31 125\n",
      "  31 125 125 125 125  94 125  31 125 125 125 125  31 125  62 125 125  31\n",
      " 125 125 125 125 125 125  62 125 125 125  62 125 125 125 125 125 125  62\n",
      " 125 125 125 125  31  31  31 125 125 125 125  62 125  62 125 125  62 125\n",
      " 125 125 125 125 125 125 125 125  31 125]\n"
     ]
    }
   ],
   "source": [
    "print(repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "early_stopping = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(EnDataset(upsample_trainset), batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_model = model_factory(\"lenet\", trainset[0][0].shape, trainset.num_classes).to(device)\n",
    "optimizer = Adam(robust_model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 580/580 [00:03<00:00, 158.28it/s, loss=0.352, acc=0.935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.42164054684894964\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 580/580 [00:03<00:00, 159.54it/s, loss=0.0448, acc=0.978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.4944981660553518\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 580/580 [00:03<00:00, 158.82it/s, loss=0.0877, acc=0.967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5711070356785595\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 580/580 [00:03<00:00, 160.80it/s, loss=0.00529, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6230410136712238\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 580/580 [00:03<00:00, 162.99it/s, loss=0.0684, acc=0.989] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6859786595531844\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 580/580 [00:03<00:00, 152.78it/s, loss=0.0589, acc=0.978] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6983994664888296\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 580/580 [00:03<00:00, 161.53it/s, loss=0.00447, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7582527509169723\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 580/580 [00:03<00:00, 159.64it/s, loss=0.00199, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7442480826942314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 580/580 [00:03<00:00, 154.93it/s, loss=0.00168, acc=1]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7249916638879627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 580/580 [00:03<00:00, 153.46it/s, loss=0.002, acc=1]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7235745248416139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 580/580 [00:03<00:00, 151.23it/s, loss=0.000454, acc=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7436645548516172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 580/580 [00:03<00:00, 161.68it/s, loss=0.0481, acc=0.989] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7562520840280094\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3333023/3517860651.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  robust_model.load_state_dict(torch.load(\"output/robust_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 12, best validation accuracy: 0.7582527509169723\n",
      "Test accuracy: 0.7724\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "kill_cnt = 0\n",
    "best_acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "    robust_model.train()\n",
    "    with tqdm(total=len(train_dataloader), dynamic_ncols=True) as t:\n",
    "        t.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x, y, idx in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = robust_model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "            acc = (y_hat.argmax(dim=1) == y).float()\n",
    "            acc = acc.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix({\n",
    "                \"loss\": loss.item(),\n",
    "                \"acc\": acc.item()\n",
    "            })\n",
    "            t.update()\n",
    "\n",
    "    robust_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, idx in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = robust_model(x)\n",
    "            correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    valid_acc = correct/total\n",
    "    print(f\"Validation accuracy: {valid_acc}\")\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        best_epoch = epoch + 1\n",
    "        kill_cnt = 0\n",
    "\n",
    "        torch.save(robust_model.state_dict(), \"output/robust_model.pth\")\n",
    "        print(\"Saving model...\")\n",
    "    else:\n",
    "        kill_cnt += 1\n",
    "        if kill_cnt == early_stopping:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "robust_model.eval()\n",
    "robust_model.load_state_dict(torch.load(\"output/robust_model.pth\"))\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y, idx in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = robust_model(x)\n",
    "        correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"Best epoch: {epoch + 1}, best validation accuracy: {best_acc}\")\n",
    "print(f\"Test accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = Evaluator(\n",
    "#     testset=testset,\n",
    "#     group_partition=testset.group_partition,\n",
    "#     group_weights=trainset.group_weights,\n",
    "#     batch_size=64,\n",
    "#     model=robust_model,\n",
    "#     device=device,\n",
    "#     verbose=True\n",
    "# )\n",
    "# evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "260D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
